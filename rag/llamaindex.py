import yaml
from datetime import datetime
import streamlit as st

from llama_index.core import Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import TextNode
from llama_index.core.vector_stores import SimpleVectorStore, VectorStoreQuery
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.llms.azure_openai import AzureOpenAI
from llama_index.readers.file import PyMuPDFReader

# === ParamÃ¨tres de dÃ©coupage ===
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100

# === Lecture de la configuration YAML ===
def read_config(file_path):
    try:
        with open(file_path, 'r') as file:
            return yaml.safe_load(file)
    except yaml.YAMLError as e:
        print(f"âŒ Erreur lecture YAML: {e}")
        return {}

config = read_config("secrets/config.yaml")

# === Initialisation LLM et Embedding ===
llm = AzureOpenAI(
    model=config["chat"]["azure_deployment"],
    deployment_name=config["chat"]["azure_deployment"],
    api_key=config["chat"]["azure_api_key"],
    azure_endpoint=config["chat"]["azure_endpoint"],
    api_version=config["chat"]["azure_api_version"],
)

embedder = AzureOpenAIEmbedding(
    model=config["embedding"]["azure_deployment"],
    deployment_name=config["embedding"]["azure_deployment"],
    api_key=config["embedding"]["azure_api_key"],
    azure_endpoint=config["embedding"]["azure_endpoint"],
    api_version=config["embedding"]["azure_api_version"],
)

Settings.llm = llm
Settings.embed_model = embedder

# === Initialisation du vecteur store partagÃ© (mÃ©morisÃ© dans la session) ===
if "llama_store" not in st.session_state:
    print("ğŸ”„ Initialisation d'un nouveau vector store LlamaIndex")
    st.session_state["llama_store"] = SimpleVectorStore()
vector_store = st.session_state["llama_store"]

# === Indexation PDF ===
def store_pdf_file(file_path: str, doc_name: str):
    print(f"ğŸ“„ Lecture du fichier : {file_path}")
    loader = PyMuPDFReader()
    documents = loader.load(file_path)
    print(f"ğŸ“ƒ {len(documents)} document(s) chargÃ©(s)")

    parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    nodes = []

    for i, doc in enumerate(documents):
        print(f"â¡ï¸ DÃ©coupage du document {i}")
        chunks = parser.split_text(doc.text)
        print(f"   ğŸ§© {len(chunks)} chunk(s) trouvÃ©(s)")

        for j, chunk in enumerate(chunks):
            try:
                embedding = embedder.get_query_embedding(chunk)  
                node = TextNode(text=chunk)
                node.embedding = embedding
                node.metadata = {
                    "document_name": doc_name,
                    "insert_date": datetime.now().isoformat()
                }
                nodes.append(node)
                print(f"     âœ… Embedding gÃ©nÃ©rÃ© pour le chunk {j}")
            except Exception as e:
                print(f"     âŒ Erreur embedding (chunk {j}): {e}")

    print(f"ğŸ—ƒï¸ Insertion de {len(nodes)} node(s) dans le vecteur store")
    vector_store.add(nodes)

# === Suppression (non supportÃ©e) ===
def delete_file_from_store(name: str):
    raise NotImplementedError("âŒ Suppression non supportÃ©e pour LlamaIndex.")

# === Recherche vectorielle ===
def retrieve(question: str, k: int = 5):
    print(f"ğŸ” Question posÃ©e : {question}")
    query_embedding = embedder.get_query_embedding(question)
    print("ğŸ“ Embedding gÃ©nÃ©rÃ© pour la question")

    query = VectorStoreQuery(
        query_embedding=query_embedding,
        similarity_top_k=k,
        mode="default"
    )

    results = vector_store.query(query)

    if results is None or not results.nodes:
        print("âš ï¸ Aucun rÃ©sultat trouvÃ© dans le vector store.")
        return []

    print(f"ğŸ“¥ {len(results.nodes)} document(s) retrouvÃ©(s)")
    return results.nodes

# === Construction du prompt ===
def build_qa_messages(question: str, context: str, language: str):
    return [
        ("system", "You are an assistant for question-answering tasks."),
        ("system", f"Respond strictly in {language}."),
        ("system", f"""Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Use three sentences maximum and keep the answer concise.
{context}"""),
        ("user", question)
    ]

# === GÃ©nÃ©ration de rÃ©ponse ===
def answer_question(question: str, language: str, k: int = 5) -> str:
    print("âš™ï¸ Appel Ã  `answer_question()`")
    docs = retrieve(question, k=k)

    if not docs:
        return "âš ï¸ Aucun document pertinent n'a Ã©tÃ© trouvÃ© dans la base de vecteurs."

    context = "\n\n".join(
        doc.text if hasattr(doc, "text") else doc.get_content(metadata_mode="all")
        for doc in docs
    )

    print("ğŸ“ Contexte extrait :")
    for i, doc in enumerate(docs):
        preview = doc.text[:200].replace("\n", " ") if hasattr(doc, "text") else doc.get_content()[:200]
        print(f"   ğŸ“„ {i}: {preview}...")

    messages = build_qa_messages(question, context, language)
    print("âœ‰ï¸ Envoi au LLM...")
    response = llm.invoke(messages)
    print("âœ… RÃ©ponse reÃ§ue")
    return response.content
